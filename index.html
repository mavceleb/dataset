<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        /* Including Monsterrat font */
        @font-face {
            font-family: 'montserrat_regular';
            src: url('./static/fonts/Montserrat/static/Montserrat-Regular.ttf') format('truetype');
        }

        @font-face {
            font-family: 'montserrat_medium';
            src: url('./static/fonts/Montserrat/static/Montserrat-Medium.ttf') format('truetype');
        }

        @font-face {
            font-family: 'montserrat_bold';
            src: url('./static/fonts/Montserrat/static/Montserrat-Bold.ttf') format('truetype');
        }

        @font-face {
            font-family: 'montserrat_lightItalic';
            src: url('./static/fonts/Montserrat/static/Montserrat-LightItalic.ttf') format('truetype');
        }

        /* //////////////////////////////////////////////////////////////////////// */


        /* Defining base properties */
        :root {
            --main-color: rgba(35, 35, 37, 1);
            --secondary-color: rgba(249, 249, 249, 1);
            --tertiary-color: rgba(230, 230, 230, 1);
            /* --secondary-color: rgba(35, 35, 37, 1);
            --main-color: rgba(249, 249, 249, 1); */
        }

        html{
            scroll-behavior: smooth;
        }

        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 0;
            font-family: montserrat_regular;
            color: black;
            font-size: 16px;
            font-family: montserrat_regular;
            text-align: justify;
        }
        /* //////////////////////////////////////////////////////////////////////// */


        /* Navbar */
        nav {
            position: fixed;
            z-index: 100;
            background-color: var(--main-color);
            height: 10vh; /* 10% of the vertical height */
            width: 100%;
            display: flex;
            justify-content: space-evenly; /* Align items to the right */
            align-items: center;
            /* padding: 0 20px; */
            font-family: montserrat_medium;
            font-size: 20px;
        }

        nav a {
            text-decoration: none;
            color: var(--secondary-color);
            margin-right: 2vw; /* Adjust the margin as needed */
        }

        nav a:hover{
            color: #e74c3c; /* Change the color on hover */
        }
        /* //////////////////////////////////////////////////////////////////// */


        /* Body container */
        .body-container{
            width: 60%;
            background-color: var(--secondary-color);
            margin: 0;
        }
        /* /////////////////////////////////////////////////////////////////// */


        /* Banner */
        .banner-container{
            display: flex;
            justify-content: center;
            align-items: center;
            width: 100%;
            height: 45vh;
            background-image: url(./static/images/Banner2.png);
            background-size: contain;
            background-position: center;
            background-repeat: no-repeat;
            background-position: top;
            margin-top: 10vh;
        }


        .card-container{
            display: flex;
            width: 350px;
            background-color: var(--secondary-color);
            opacity: 0.85; /* Set the desired opacity value */
            height: 100px;
            padding: 10px;
            justify-content: center;
            align-items: center;
            text-align: center;
            font-size: 26px;
            font-family: montserrat_medium;
            border-radius: 10px;
            box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.6); /* Set the box shadow */
            transition: transform 0.3s;

            /* background-color: #e74c3c; */
        }

        .card-container:hover{
            transform: scale(1.02);
        }
        /* ///////////////////////////////////////////////////////////////////// */


        /* Description data */
        .descriptions{
            padding: 0 40px;
        }

        .description-headings{
            font-family: montserrat_medium;
            font-size: 24px;
            padding-top: 8vh;
            padding-bottom: 2vh;
            text-align: center;
            margin: 0;
        }

        .figures{
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            font-family: montserrat_lightItalic;
            font-size: 12px;
            text-align: center;
        }

        .figures img{
            width: 80%;
        }

        .figures p{
            width: 80%;
        }
        /* ////////////////////////////////////////////////////////////////////// */

        
        /* Publications */
        .publications-container{
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
        }
        /* /////////////////////////////////////////////////////////////////// */


        /* Challenge button */
        .challenge-button-div{
            display: flex;
            justify-content: center;
            align-items: center;
        }
        
        .challenge-button{
            border-radius: 50px;
            /* height: 7vh; */
            padding: 10px;
            width: 40%;
            background-color: #e74c3c;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: transform 0.3s;
        }

        .challenge-button:hover{
            transform: scale(1.02);
            cursor: pointer;
        }
        /* /////////////////////////////////////////////////////////////////// */

        .footer{
            margin-top: 10vh;
            width: 100%;
            height: 5vh;
            background-color: var(--main-color);
        }
        
        @media (max-width: 900px) {
            .body-container{
                width: 100%;
            }

            .banner-container{
                height: 30vh;
            }

            .challenge-button{
                width: 80%;
            }
        }
    </style>
</head>
<body>

    <!-- Navbar -->
    <nav>
        <div>
            <a href="./index.html">Home</a>
            <a href="#introduction">Introduction</a>
            <a href="#dataset">Dataset</a>
            <a href="#publications">Publications</a>
            <a href="#challenge">Challenge</a>
        </div>
    </nav>
    <!-- //////////////////////////////////////////////////////////////////////// -->

    
    <div class="body-container">
        <!-- banner -->
        <div class="banner-container">
            <div class="card-container">
                <p>
                    <span style="font-family: montserrat_medium;"> MAV-CELEB </span><br> 
                    <span style="font-size: 14px;"> Multi-lingual Audio Visual dataset of Celebrities </span>
                </p>
                <p></p>
            </div>
        </div>
        <!-- //////////////////////////////////////////////////////////////////////// -->

        <!-- Introduction -->
        <div class="descriptions" id="introduction">
            <p class="description-headings">INTRODUCTION</p>
            <p>
                Machines achieving human-like face-voice association ability is expected to significantly enhance 
                the efficacy of multimodal biometrics, particularly amid the rising tide of disinformation fueled 
                by the powerful weapon of audio-vision deepfakes. Prior works adopt metric learning methods to 
                learn an embedding space that is amenable to associated matching and verification tasks. 
                <br> <br>
                Albeit showing some progress, such formulations are, however, restrictive due to dependency on distance-dependent 
                margin parameter, poor runtime training complexity, and reliance on carefully crafted negative mining 
                procedures. In this work,we hypothesize that an enriched representation coupled with effective yet 
                efficient supervision is important towards realizing a discriminative joint embedding space for 
                face-voice association tasks. To this end, we propose a light-weight, plug-and-play mechanism that 
                exploits the complementary cues in both modalities to form enriched fused embeddings and clusters 
                them based on their identity labels via orthogonality constraints.
                <br> <br>
                We coin our proposed mechanism as fusion and orthogonal projection (FOP) and instantiate in a 
                two-stream network. The overall resulting framework is evaluated on VoxCeleb1 and MAV-Celeb datasets 
                with a multitude of tasks, including cross-modal verification and matching. Results reveal that our 
                method performs favourably against the current state-of-the-art methods and our proposed formulation of 
                supervision is more effective and efficient than the ones employed by the contemporary methods. 
                Lastly, we leverage cross-modal verification and matching tasks to analyze the impact of multiple 
                languages on face-voice association.
            </p>
        </div>
        <div class="figures">
            <img src="./static/images/Workflow.png">
            <p>
                Figure 1: Diagram showing cross-modal verification and matching task on Face-Voice Association with multiple languages.
            </p>
        </div>
        <!-- //////////////////////////////////////////////////////////////////////// -->

        <!-- Dataset -->
        <div class="descriptions" id="dataset">
            <p class="description-headings">DATASET</p>
            <p>
                The data is obtained from YouTube videos, consisting of celebrity interviews along with talk shows, 
                and television debates. The visual data spans over a vast range of variations including poses, 
                motion blur, background clutter, video quality, occlusions and lighting conditions. Moreover, most videos 
                contain real-world noise like background chatter, music, over-lapping speech, and compression artifacts, 
                resulting into a challenging dataset to evaluate multimedia systems.
            </p>
            <p>The dataset is available on the following links:</p>
            <ul>
                <li>MAV-CELEB v1</li>
                <ul>
                    <li>Raw files: <span><a href="https://github.com/mavceleb/dataset">https://github.com/mavceleb/dataset</a></span></li>
                    <li>CSV files: <span><a href="https://github.com/mavceleb/dataset">https://github.com/mavceleb/dataset</a></span></li>
                </ul>
                <li>MAV-CELEB v2</li>
                <ul>
                    <li>Raw files: <span><a href="https://github.com/mavceleb/dataset">https://github.com/mavceleb/dataset</a></span></li>
                    <li>CSV files: <span><a href="https://github.com/mavceleb/dataset">https://github.com/mavceleb/dataset</a></span></li>
                </ul>
            </ul>
        </div>
        <!-- //////////////////////////////////////////////////////////////////////// -->

        <!-- Publications -->
        <div class="descriptions" id="publications" style="background-color: var(--tertiary-color);">
            <p class="description-headings">PUBLICATIONS</p>
            <div class="publications-container">
                
                <p>
                    <b>Dataset Paper</b>
                </p>
                <a href="https://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Nawaz_Cross-Modal_Speaker_Verification_and_Recognition_A_Multilingual_Perspective_CVPRW_2021_paper.pdf"
                style="font-size: 14px; margin-bottom: 15px;">
                    Link to the paper
                </a>
                <div style="width: 60%; display: flex; flex-direction: column; font-size: 14px;">
                    <span>
                        @inproceedings{nawaz2021cross,
                    </span>
                    <span style="padding-left: 50px;">
                            title={Cross-modal speaker verification and recognition: A multilingual perspective}, <br>
                            author={Nawaz, Shah and Saeed, Muhammad Saad and Morerio, Pietro and Mahmood, Arif and Gallo, Ignazio and Yousaf, Muhammad Haroon and Del Bue, Alessio}, <br>
                            booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, <br>
                            pages={1682--1691},<br>
                            year={2021}
                    </span>
                    <span>}</span>
                </div>
                <p>
                    <b>Baseline Paper</b>
                </p>
                <a href="https://ieeexplore.ieee.org/abstract/document/9747704/"
                style="font-size: 14px; margin-bottom: 15px;">
                    Link to the paper
                </a>
                <div style="width: 60%; display: flex; flex-direction: column; font-size: 14px;">
                    <span>
                        @inproceedings{saeed2022fusion,
                    </span>
                    <span style="padding-left: 50px;">
                        title={Fusion and orthogonal projection for improved face-voice association}, <br>
                        author={Saeed, Muhammad Saad and Khan, Muhammad Haris and Nawaz, Shah and Yousaf, Muhammad Haroon and Del Bue, Alessio}, <br>
                        booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, <br>
                        pages={7057--7061}, <br>
                        year={2022}, <br>
                        organization={IEEE} <br>
                    </span>
                    <span style="padding-bottom: 2vh;">}</span>
                </div>
            </div>
        </div>
        <!-- /////////////////////////////////////////////////////////////////////// -->

        <!-- Challenge -->
        <div class="descriptions" id="challenge">
            <p class="description-headings">CHALLENGE</p>
            <p>The FAME Challenge focuses on Speaker Recognition in a multi-modal (face and voice) perspective. 
                The complexity of the task increases further when we consider multiple spoken languages for the 
                vocal data. This introduces a domain gap as the predictive models fail to establish a strong 
                relationship across different languages for the speakers. The aim of the challenge is to develop 
                techniques that tackle this domain gap and improve results on the Speaker Recognition task using 
                multi-modal and cross-language techniques.</p>
                <br>
                <p style="text-align: center;">Click to see more details!</p>
        </div>
        <a href="./competition.html" style="all: initial;">
            <div class="challenge-button-div">
                <div class="challenge-button">
                    <p class="description-headings" style="padding: 0; margin: 0%; color: white;">FAME Challenge 2024</p>
                </div>
            </div>
        </a>
        <!-- ///////////////////////////////////////////////////////////////////////// -->
        
        <!-- Footer -->
        <div class="footer">

        </div>
    </div>
    

</body>
</html>