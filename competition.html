<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        /* Including Monsterrat font */
        @font-face {
            font-family: 'montserrat_regular';
            src: url('./static/fonts/Montserrat/static/Montserrat-Regular.ttf') format('truetype');
        }

        @font-face {
            font-family: 'montserrat_medium';
            src: url('./static/fonts/Montserrat/static/Montserrat-Medium.ttf') format('truetype');
        }

        @font-face {
            font-family: 'montserrat_bold';
            src: url('./static/fonts/Montserrat/static/Montserrat-Bold.ttf') format('truetype');
        }

        @font-face {
            font-family: 'montserrat_lightItalic';
            src: url('./static/fonts/Montserrat/static/Montserrat-LightItalic.ttf') format('truetype');
        }

        /* //////////////////////////////////////////////////////////////////////// */


        /* Defining base properties */
        :root {
            --main-color: rgba(35, 35, 37, 1); /* Define a variable for the main color */
            --secondary-color: rgba(249, 249, 249, 1); /* Define a variable for the secondary color */
            --tertiary-color: rgba(230, 230, 230, 1);
        }

        html{
            scroll-behavior: smooth;
        }

        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 0;
            font-family: montserrat_regular;
            color: black;
            font-size: 16px;
            font-family: montserrat_regular;
            text-align: justify;
        }
        /* //////////////////////////////////////////////////////////////////////// */


        /* Navbar */
        nav {
            position: fixed;
            z-index: 100;
            background-color: var(--main-color);
            height: 10vh; /* 10% of the vertical height */
            width: 100%;
            display: flex;
            justify-content: space-evenly; /* Align items to the right */
            align-items: center;
            /* padding: 0 20px; */
            font-family: montserrat_medium;
            font-size: 20px;
        }

        nav a {
            text-decoration: none;
            color: var(--secondary-color);
            margin-right: 2vw; /* Adjust the margin as needed */
        }

        nav a:hover{
            color: #e74c3c; /* Change the color on hover */
        }
        /* //////////////////////////////////////////////////////////////////// */


        /* Body container */
        .body-container{
            width: 60%;
            background-color: var(--secondary-color);
        }
        /* /////////////////////////////////////////////////////////////////// */


        /* Banner */
        .banner-container{
            display: flex;
            justify-content: center;
            align-items: center;
            width: 100%;
            height: 45vh;
            background-image: url(./static/images/Banner2.png);
            background-size: contain;
            background-position: center;
            background-repeat: no-repeat;
            background-position: top;
            margin-top: 10vh;
        }


        .card-container{
            display: flex;
            width: 350px;
            background-color: var(--secondary-color);
            opacity: 0.85; /* Set the desired opacity value */
            height: 80px;
            justify-content: center;
            align-items: center;
            text-align: center;
            font-size: 26px;
            font-family: montserrat_medium;
            border-radius: 10px;
            box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.6); /* Set the box shadow */
            transition: transform 0.3s;
        }

        .card-container:hover{
            transform: scale(1.02);
        }
        /* ///////////////////////////////////////////////////////////////////// */


        /* Description data */
        .descriptions{
            padding: 0 40px;
        }

        .description-headings{
            font-family: montserrat_medium;
            font-size: 24px;
            margin: 0;
            padding-top: 8vh;
            padding-bottom: 2vh;
        }

        .figures{
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            font-family: montserrat_lightItalic;
            font-size: 12px;
            text-align: center;
        }

        .figures img{
            width: 80%;
        }
        /* ////////////////////////////////////////////////////////////////////// */

        /* Dataset Table */
        .table-container {
            width: 80%;
            margin: 10px auto;
            /* background-color: #fff; */
            /* box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); */
            display: flex;
            flex-direction: column;
        }

        .table-container div div{
            text-align: center;
        }

        .table-header {
            background-color: var(--main-color);
            color: white;
            display: flex;
        }

        .table-header div {
            flex: 1;
            padding: 10px;
            text-align: left;
        }

        .table-body {
            display: flex;
            flex-direction: column;
        }

        .table-row {
            display: flex;
        }

        .table-cell {
            flex: 1;
            padding: 10px;
            /* border: 1px solid var(--main-color); */
            text-align: left;
        }

        .table-row:nth-child(even) {
            background-color: rgb(175, 175, 175);
        }
        /* ////////////////////////////////////////////////////////////////// */

        .footer{
            margin-top: 10vh;
            width: 100%;
            height: 5vh;
            background-color: var(--main-color);
        }


        @media (max-width: 900px) {
            .body-container{
                width: 100%;
            }

            .banner-container{
                height: 30vh;
            }
        }
    </style>
</head>
<body>

    <!-- Navbar -->
    <nav>
        <div>
            <a href="./index.html">Home</a>
            <a href="#dataset">Dataset</a>
            <a href="#baseline">Baseline</a>
            <a href="#task">Task</a>
            <a href="#evaluation">Evaluation</a>
            <a href="#registration">Registration</a>
            <a href="#organizers">Organizers</a>
            
        </div>
    </nav>
    <!-- //////////////////////////////////////////////////////////////////////// -->

    <!-- banner -->
    <div class="body-container">
        <div class="banner-container">
            <div class="card-container">
                FAME Challenge 2024
            </div>
        </div>
        <!-- //////////////////////////////////////////////////////////////////////// -->

        <!-- Introduction -->
        <div class="descriptions">
            <p>
                The face and voice of a person have unique characteristics and they are well used as biometric 
                measures for person authentication either as a unimodal or multimodal. A strong correlation has 
                been found between face and voice of a person, which has attracted significant research interest. 
                Though previous works have established association between faces and voices, none of these approaches 
                investigated the effect of multiple languages on this task. As half of the population of world is 
                bilingual and we are more often communicating in multilingual scenarios [10], therefore, it is essential 
                to investigate the effect of language for associating faces with the voices. Thus, the goal of the 
                <b>Face-voice Association in Multilingual Environments (FAME)</b> challenge is to analyze the impact of multiple 
                languages on face-voice association task.
            </p>
        </div>

        <!-- Dataset -->
        <div class="descriptions" id="dataset" style="background-color: var(--tertiary-color);">
            <p class="description-headings">DATASET</p>
            <p>
                Our dataset comprises of two versions, MAV-CELEB v1 and MAV-CELEB v2 both containing 
                different (non-inclusive) speaker identities. The v1 contains audio visual data of speakers
                 with Urdu and English languages while the v2 contains speakers with Hindi and English languages.
            </p>
            <ul>
                <li>Facial images</li>
                <li>Voice recordings</li>
                <ul>
                    <li>Urdu (v1 only)</li>
                    <li>English (v1 and v2)</li>
                    <li>Hindi (v2 only)</li>
                </ul>
            </ul>
            <p>The data is available in two formats:</p>
            <ul>
                <li>Raw Images and audio file.</li>
                <li>CSV files of image and audio features extracted through a pretrained
                     feature extraction model (utterance level aggregator for voice and vggface for images).</li>
            </ul>
            
            <p>The dataset is available on the following links:</p>
            <ul>
                <li>MAV-CELEB v1</li>
                <ul>
                    <li>Raw files: <span><a href="https://github.com/mavceleb/dataset">https://github.com/mavceleb/dataset</a></span></li>
                    <li>CSV files: <span><a href="https://github.com/mavceleb/dataset">https://github.com/mavceleb/dataset</a></span></li>
                </ul>
                <li>MAV-CELEB v2</li>
                <ul>
                    <li>Raw files: <span><a href="https://github.com/mavceleb/dataset">https://github.com/mavceleb/dataset</a></span></li>
                    <li>CSV files: <span><a href="https://github.com/mavceleb/dataset">https://github.com/mavceleb/dataset</a></span></li>
                </ul>
            </ul>

            <div class="table-container" style="padding-bottom: 2vh;">
                <div class="table-header">
                    <div style="text-align: center;"></div>
                    <div>MAV-CELEB v1</div>
                    <div>MAV-CELEB v2</div>
                </div>
                <div class="table-body">
                    <div class="table-row">
                        <div class="table-cell">Languages</div>
                        <div class="table-cell">E / U / EU</div>
                        <div class="table-cell">E / H / EH</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell"># of celebs</div>
                        <div class="table-cell">70</div>
                        <div class="table-cell">84</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell"># of male celebs</div>
                        <div class="table-cell">43</div>
                        <div class="table-cell">56</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell"># of female celebs</div>
                        <div class="table-cell">27</div>
                        <div class="table-cell">28</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell"># of videos</div>
                        <div class="table-cell">402 / 555 / 957</div>
                        <div class="table-cell">646 / 484 1130</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell"># of hours</div>
                        <div class="table-cell">30 / 54 / 84</div>
                        <div class="table-cell">51 / 33 / 84</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell"># of utterances</div>
                        <div class="table-cell">6850 / 12706 / 19556</div>
                        <div class="table-cell">12579 / 8136 / 20715</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell">Avg# of videos per celebrity</div>
                        <div class="table-cell">6 / 8 / 14</div>
                        <div class="table-cell">8 / 6 / 14</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell">Avg# of utterances per celebrity</div>
                        <div class="table-cell">98 / 182 / 280</div>
                        <div class="table-cell">150 / 97 / 247</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell">Avg length of utterance(s)</div>
                        <div class="table-cell">15.8 / 15.3 / 15.6</div>
                        <div class="table-cell">14.6 / 14.6 / 14.6</div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Baseline Model -->
        <div class="descriptions" id="baseline">
            <p class="description-headings">BASELINE MODEL</p>
            <p>
                We provide a baseline model that has been trained on extracted features for facial and 
                audio data (vggface for images and utterance level aggregator for voices). To learn a discriminative 
                joint face-voice embedding for F-V association tasks, we develop a new framework for crossmodal face-voice 
                association (See Fig. 1) that is fundamentally a two-stream pipeline and features a light-weight 
                module that exploits complementary cues from both face and voice embeddings and facilitates discriminative identity
                mapping via orthogonality constraints
                <br> <br>
                Link to the paper: 
                <a href="https://ieeexplore.ieee.org/abstract/document/9747704/">
                    Fusion and Orthogonal Projection for Improved Face-Voice Association
                </a>
                <br>
                Link to the code: 
                <a href="https://github.com/msaadsaeed/FOP">
                    https://github.com/msaadsaeed/FOP
                </a>
            </p>
        </div>
        <div class="figures">
            <img src="./static/images/methodology.png">
            <p>Figure 1: Diagram showing our methodology.</p>
        </div>

        <!-- Task -->
        <div class="descriptions" id="task">
            <p class="description-headings">TASK</p>
            <p><b>Cross-modal Verification and Matching</b></p>
            <p>
                Face-voice association is established in cross-modal verification and matching tasks. The 
                goal of the cross-modal verification task is to verify if, in a given single sample with both a face 
                and voice, both belong to the same identity. In a cross-modal matching task, Voice-Face(V-F): given a 
                clip of a voice  and two or more face images, select the face image that belongs to the voice. 
                (Face-Voice) F-V: given an image of a face, determine the respective voice. In addition, we analyze 
                the impact of multiple of languages on cross-modal verification and matching tasks.
            </p>
            <div class="figures">
                <img src="./static/images/Workflow.png">
                <p>Figure 2: Diagram explaining cross-modal verification and matching task in face-voice association.</p>
            </div>
        </div>

        <!-- Evaluation Metrics -->
        <div class="descriptions" id="evaluation" style="background-color: var(--tertiary-color); padding-bottom: 2vh;">
            <p class="description-headings">EVALUATION METRICS</p>
            <p>
                The training and validation sets will be shared to the participants on the first day of the progress 
                phase, which they can use to build their systems. A pretrained model will also be shared during this 
                period. The evaluation set without the keys will be shared on the first day of the evaluation phase, 
                which the participants can use to test using their trained models that they developed during the progress phase.
            </p>
            <p>We measure the results on metrics:</p>
            <ul>
                <li>Area under the ROC Curve (AUC)</li>
                <li>Equal Error Rate (EER)</li>
            </ul>
            <p>on both seen-heard and unseen-unheard cases.</p>
        </div>

        <!-- Registration -->
        <div class="descriptions" id="registration">
            <p class="description-headings">REGISTRATION</p>
            <p>We welcome participants to apply for the “FAME Challenge 2023” by expressing their interest via google forms at 
                <span><a href="https://docs.google.com/forms/d/e/1FAIpQLScBPcRr_ReZHpGE1mxE1Zajjyg5chwOzQtqhV3KCmKBSoOKBA/viewform?gxid=-8203366">this link</a></span>.</p>
        </div>

        <!-- Organizers -->
        <div class="descriptions" id="organizers">
            <p class="description-headings">ORGANIZERS</p>
            <p>
                <a href="https://scholar.google.com/citations?user=uyhEJ5IAAAAJ&hl=en">Muhammad Saad Saeed</a> - <span  style="font-size: 12px;">Swarm Robotics Lab (SRL)-NCRA, University of Engineering and Technology Taxila</span> <br>
                <a href="https://shahnawazgrewal.github.io/">Shah Nawaz</a> - <span  style="font-size: 12px;">Johannes Kepler University Linz</span> <br>
                <a href="#">Muhammad Salman Tahir</a> - <span  style="font-size: 12px;">Swarm Robotics Lab (SRL)-NCRA, University of Engineering and Technology Taxila</span> <br>
                <a href="https://sites.google.com/view/rohankumardas">Rohan Kumar Das</a> - <span  style="font-size: 12px;">Fortemedia Singapore, Singapore</span> <br>
                <a href="https://scholar.google.com/citations?user=nFxWrXEAAAAJ&hl=en">Muhammad Zaigham Zaheer</a> - <span  style="font-size: 12px;">Mohamed bin Zayed University of Artificial Intelligence</span> <br>
                <a href="https://hcai.at/persons/moscati/">Marta Moscati</a> - <span  style="font-size: 12px;">Institute of Computational Perception, Johannes Kepler University Linz, Austria</span> <br>
                <a href="http://www.mschedl.eu/">Markus Schedl</a> - <span  style="font-size: 12px;">Institute of Computational Perception, Johannes Kepler University Linz, Austria | Human-centered AI Group, AI Lab, Linz Institute of Technology, Austria</span> <br>
                <a href="https://mbzuai.ac.ae/study/faculty/muhammad-haris-khan/">Muhammad Haris Khan</a> - <span  style="font-size: 12px;">Mohamed bin Zayed University of Artificial Intelligence</span> <br>
                <a href="https://mbzuai.ac.ae/study/faculty/karthik-nandakumar/">Karthik Nandakumar</a> - <span  style="font-size: 12px;">Mohamed bin Zayed University of Artificial Intelligence</span> <br>
                <a href="https://fms.uettaxila.edu.pk/Profile/haroon.yousaf">Muhammad Haroon Yousaf</a> - <span  style="font-size: 12px;">Swarm Robotics Lab (SRL)-NCRA, University of Engineering and Technology Taxila</span> <br>
                </p>
        </div>

        <!-- footer -->
        <div class="footer">

        </div>
    </div>
    

</body>
</html>