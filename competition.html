<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        /* Including Monsterrat font */
        @font-face {
            font-family: 'montserrat_regular';
            src: url('./static/fonts/Montserrat/static/Montserrat-Regular.ttf') format('truetype');
            /* src: url('./static/fonts/roboto/Roboto-Regular.ttf') format('truetype'); */
        }

        @font-face {
            font-family: 'montserrat_medium';
            src: url('./static/fonts/Montserrat/static/Montserrat-Medium.ttf') format('truetype');
            /* src: url('./static/fonts/roboto/Roboto-Medium.ttf') format('truetype'); */
        }

        @font-face {
            font-family: 'montserrat_bold';
            src: url('./static/fonts/Montserrat/static/Montserrat-Bold.ttf') format('truetype');
            /* src: url('./static/fonts/roboto/Roboto-Bold.ttf') format('truetype'); */
        }

        @font-face {
            font-family: 'montserrat_lightItalic';
            src: url('./static/fonts/Montserrat/static/Montserrat-LightItalic.ttf') format('truetype');
            /* src: url('./static/fonts/roboto/Roboto-LightItalic.ttf') format('truetype'); */
        }

        /* //////////////////////////////////////////////////////////////////////// */


        /* Defining base properties */
        :root {
            /* --main-color: rgba(35, 35, 37, 1); */
            --main-color: #0d0c1d;
            --secondary-color: rgba(249, 249, 249, 1);
            --tertiary-color: rgba(230, 230, 230, 1);
            /* --tertiary-color: #d5e3f1; */
            /* --secondary-color: rgba(35, 35, 37, 1);
            --main-color: rgba(249, 249, 249, 1); */
        }

        html{
            scroll-behavior: smooth;
        }

        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 0;
            font-family: montserrat_regular;
            color: black;
            background-image: linear-gradient(rgba(0, 0, 0, 0.4), rgba(0, 0, 0, 0.4)), url("static/images/bg_image.jpg");
            background-attachment: fixed;
            background-position: center;
            font-size: 16px;
            font-family: montserrat_regular;
            text-align: justify;
        }
        /* //////////////////////////////////////////////////////////////////////// */


        /* Navbar */
        nav {
            position: fixed;
            z-index: 100;
            background-color: var(--main-color);
            height: 10vh;
            width: 100%;
            display: flex;
            justify-content: space-evenly;
            align-items: center;
            font-family: montserrat_medium;
            font-size: 20px;
        }

        nav a {
            text-decoration: none;
            color: var(--secondary-color);
            margin-right: 2vw; /* Adjust the margin as needed */
        }

        nav a:hover{
            color: #e74c3c; /* Change the color on hover */
        }
        /* //////////////////////////////////////////////////////////////////// */


        /* Body container */
        .body-container{
            width: 60%;
            background-color: var(--secondary-color);
        }
        /* /////////////////////////////////////////////////////////////////// */


        /* Banner */
        .banner-container{
            display: flex;
            justify-content: center;
            align-items: center;
            width: 100%;
            height: 40vh;
            /* background-image: url(./static/images/Banner2.png); */
            background-image: linear-gradient(rgba(0, 0, 0, 0.2), rgba(0, 0, 0, 0.2)), url("./static/images/Banner2.png");
            background-size: contain;
            background-position: center;
            background-repeat: no-repeat;
            background-position: top;
            margin-top: 10vh;
        }


        .card-container{
            display: flex;
            width: 350px;
            padding: 10px;
            background-color: var(--secondary-color);
            opacity: 0.85; /* Set the desired opacity value */
            height: 80px;
            justify-content: center;
            align-items: center;
            text-align: center;
            font-size: 28px;
            font-family: montserrat_bold;
            border-radius: 10px;
            box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.6); /* Set the box shadow */
            transition: transform 0.3s;
        }

        .card-container:hover{
            transform: scale(1.02);
        }
        /* ///////////////////////////////////////////////////////////////////// */


        /* Description data */
        .descriptions{
            padding: 0 40px;
        }

        .description-headings{
            font-family: montserrat_bold;
            font-size: 28px;
            margin: 0;
            padding-top: 8vh;
            padding-bottom: 2vh;
        }

        .figures{
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            font-family: montserrat_lightItalic;
            font-size: 12px;
            text-align: center;
        }

        .figures img{
            width: 80%;
        }
        /* ////////////////////////////////////////////////////////////////////// */

        /* Challenge winners table */
        table {
            border-collapse: collapse;
            width: 80%;
            margin: 20px auto;
            box-shadow: 0 5px 10px #e1e1e1;
            border-radius: 5px 5px 0 0;
            overflow: hidden;
        }

        table thead tr {
            background-color: var(--main-color);
            color: #ffffff;
            text-align: left;
            font-weight: bold;
        }

        table th,
        table td {
            padding: 12px 15px;
        }

        table tbody tr {
            border-bottom: 1px solid #dddddd;
        }

        table tbody tr:nth-of-type(even) {
            background-color: #f3f3f3;
        }

        table tbody tr:last-of-type {
            border-bottom: 2px solid var(--main-color);
        }

        table tbody tr.active-row {
            font-weight: bold;
            color: var(--main-color);
        }
        /* -------------------------------- */

        
        /* Dataset */
        .dataset-container{
            display: flex;
            width: 100%;
            justify-content: space-between;
            margin-bottom: 20px;
        }

        .dataset-item{
            display: flex;
            flex-direction: column;
            width: 48%;
            text-align: left;
            height: 45vh;
        }

        /* Dataset Table */
        .table-container {
            width: 80%;
            margin: 10px auto;
            /* background-color: #fff; */
            /* box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); */
            display: flex;
            flex-direction: column;
        }

        .table-container div div{
            text-align: center;
        }

        .table-header {
            background-color: var(--main-color);
            color: white;
            display: flex;
        }

        .table-header div {
            flex: 1;
            padding: 10px;
            text-align: left;
        }

        .table-body {
            display: flex;
            flex-direction: column;
        }

        .table-row {
            display: flex;
        }

        .table-cell {
            flex: 1;
            padding: 10px;
            /* border: 1px solid var(--main-color); */
            text-align: left;
        }

        .table-row:nth-child(even) {
            background-color: rgb(175, 175, 175);
        }
        /* ////////////////////////////////////////////////////////////////// */

        .footer{
            margin-top: 10vh;
            width: 100%;
            height: 5vh;
            background-color: var(--main-color);
        }


        @media (max-width: 1200px) {
            .body-container{
                width: 100%;
            }
        }

        @media (max-width: 900px) {
            .banner-container{
                height: 30vh;
            }

            .dataset-container{
                flex-direction: column;
            }

            .dataset-item{
                width: 100%;
            }
        }
    </style>
</head>
<body>

    <!-- Navbar -->
    <nav>
        <div>
            <a href="./index.html">Home</a>
            <a href="#dataset">Dataset</a>
            <a href="#baseline">Baseline</a>
            <a href="#task">Task</a>
            <a href="#evaluation">Evaluation</a>
            <a href="#submission">Submission</a>
            <a href="#registration">Registration</a>
            <a href="#organizers">Organizers</a>
            
        </div>
    </nav>
    <!-- //////////////////////////////////////////////////////////////////////// -->

    <!-- banner -->
    <div class="body-container">
        <div class="banner-container">
            <div class="card-container">
                FAME Challenge 2024
            </div>
        </div>
        <!-- //////////////////////////////////////////////////////////////////////// -->

        <!-- Introduction -->
        <div class="descriptions">
            <p style="margin-bottom: 20px;">
                The face and voice of a person have unique characteristics and they are well used as biometric 
                measures for person authentication either as a unimodal or multimodal. A strong correlation has 
                been found between face and voice of a person, which has attracted significant research interest. 
                Though previous works have established association between faces and voices, none of these approaches 
                investigated the effect of multiple languages on this task. As half of the population of world is 
                bilingual and we are more often communicating in multilingual scenarios, therefore, it is essential 
                to investigate the effect of language for associating faces with the voices. Thus, the goal of the 
                <b>Face-voice Association in Multilingual Environments (FAME) 2024</b> challenge is to analyze the impact of multiple 
                languages on face-voice association task. For more information on challenge please see <a href="https://arxiv.org/abs/2404.09342">evaluation plan.</a>
            </p>
        </div>

        <!-- Challenge Winners -->
        <div class="descriptions" id="winners">
            <p class="description-headings">CHALLENGE WINNERS</p>
            <p style="margin-bottom: 20px;">
                Results are now announced. Congratulations to the winners!
            </p>
            <table class="styled-table">
                <thead>
                    <tr>
                        <th>Rank</th>
                        <th>Team Name</th>
                        <th>Primary Contact</th>
                        <th>Affiliation</th>
                        <th>Score (EER)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1</td>
                        <td>HLT</td>
                        <td>Tao Ruijie</td>
                        <td>National University of Singapore</td>
                        <td>19.91</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>Audio_Visual</td>
                        <td>Wuyang Chen</td>
                        <td>National University of Defense Technology</td>
                        <td>20.51</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>Xaiofei</td>
                        <td>Tang Jie Hui</td>
                        <td>Hefei University of Technology</td>
                        <td>21.76</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <!-- Dataset -->
        <div class="descriptions" id="dataset" style="background-color: var(--tertiary-color);">
            <p class="description-headings">DATASET</p>
            <p>
                Our dataset comprises of two versions, MAV-CELEB v1 and MAV-CELEB v2 both containing 
                different (non-inclusive) speaker identities. The v1 contains audio visual data of speakers
                 with Urdu and English languages while the v2 contains speakers with Hindi and English languages.
            </p>
            <ul>
                <li>Facial images</li>
                <li>Voice recordings</li>
                <ul>
                    <li>Urdu (v1 only)</li>
                    <li>English (v1 and v2)</li>
                    <li>Hindi (v2 only)</li>
                </ul>
            </ul>
            <div class="dataset-container">
                <div class="dataset-item">
                    <p>The dataset is available on the following links:</p>
                    <ul>
                        <li>MAV-CELEB v1</li>
                        <ul>
                            <li><a href="https://drive.google.com/drive/folders/1OJyjXJULErvrvzLQmpJn5v8rRo0n_fod?usp=sharing">Raw files link</a></li>
                            <li><a href="https://drive.google.com/drive/folders/1MEHtEVh9lSa9hNZxjEfNJnE3qrpm_PKw?usp=sharing">Train/Test Split</a></li>
                        </ul>
                        <li>MAV-CELEB v2</li>
                        <ul>
                            <li><a href="https://drive.google.com/drive/folders/1OJyjXJULErvrvzLQmpJn5v8rRo0n_fod?usp=sharing">Raw files link</a></li>
                            <li><a href="https://drive.google.com/drive/folders/1MEHtEVh9lSa9hNZxjEfNJnE3qrpm_PKw?usp=sharing">Train/Test Split</a></li>
                        </ul>
                    </ul>
                    <p>
                        To view the meta-data for the dataset, you can view the PDFs attached below:
                        <ul>
                            <li><a href="./static/docs/v1_meta.pdf">v1 meta-data file</a></li>
                            <li><a href="./static/docs/v2_meta.pdf">v2 meta-data file</a></li>
                        </ul>
                    </p>
                </div>

                <div class="dataset-item" style="overflow-y: scroll;">
                    <p>The file structure is like:</p>
                    <img src="./static/images/dataset_structure_2.png">
                </div>

            </div>

            <div class="table-container" style="padding-bottom: 2vh;">
                <div class="table-header">
                    <div style="text-align: center;"></div>
                    <div>MAV-CELEB v1</div>
                    <div>MAV-CELEB v2</div>
                </div>
                <div class="table-body">
                    <div class="table-row">
                        <div class="table-cell">Languages</div>
                        <div class="table-cell">E / U / EU</div>
                        <div class="table-cell">E / H / EH</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell"># of celebs</div>
                        <div class="table-cell">70</div>
                        <div class="table-cell">84</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell"># of male celebs</div>
                        <div class="table-cell">43</div>
                        <div class="table-cell">56</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell"># of female celebs</div>
                        <div class="table-cell">27</div>
                        <div class="table-cell">28</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell"># of videos</div>
                        <div class="table-cell">402 / 555 / 957</div>
                        <div class="table-cell">646 / 484 1130</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell"># of hours</div>
                        <div class="table-cell">30 / 54 / 84</div>
                        <div class="table-cell">51 / 33 / 84</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell"># of utterances</div>
                        <div class="table-cell">6850 / 12706 / 19556</div>
                        <div class="table-cell">12579 / 8136 / 20715</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell">Avg# of videos per celebrity</div>
                        <div class="table-cell">6 / 8 / 14</div>
                        <div class="table-cell">8 / 6 / 14</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell">Avg# of utterances per celebrity</div>
                        <div class="table-cell">98 / 182 / 280</div>
                        <div class="table-cell">150 / 97 / 247</div>
                    </div>
                    <div class="table-row">
                        <div class="table-cell">Avg length of utterance(s)</div>
                        <div class="table-cell">15.8 / 15.3 / 15.6</div>
                        <div class="table-cell">14.6 / 14.6 / 14.6</div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Baseline Model -->
        <div class="descriptions" id="baseline">
            <p class="description-headings">BASELINE MODEL</p>
            <p>
                We provide a baseline model that has been trained on extracted features for facial and 
                audio data (vggface for images and utterance level aggregator for voices). To learn a discriminative 
                joint face-voice embedding for F-V association tasks, we develop a new framework for crossmodal face-voice 
                association (See Fig. 1) that is fundamentally a two-stream pipeline and features a light-weight 
                module that exploits complementary cues from both face and voice embeddings and facilitates discriminative identity
                mapping via orthogonality constraints
                <br> <br>
                Link to the paper: 
                <a href="https://ieeexplore.ieee.org/abstract/document/9747704/">
                    Fusion and Orthogonal Projection for Improved Face-Voice Association
                </a>
                <br>
                Link to the Paper's code: 
                <a href="https://github.com/msaadsaeed/FOP">
                    https://github.com/msaadsaeed/FOP
                </a>
                <br>
                Link to the Baseline code: 
                <a href="https://github.com/mavceleb/mavceleb_baseline">
                    https://github.com/mavceleb/mavceleb_baseline
                </a>
            </p>
        </div>
        <div class="figures">
            <img src="./static/images/methodology.png">
            <p>Figure 1: Diagram showing our methodology.</p>
        </div>

        <!-- Task -->
        <div class="descriptions" id="task">
            <p class="description-headings">TASK</p>
            <p><b>Cross-modal Verification</b></p>
            <p>
                Face-voice association is established in cross-modal verification task. The 
                goal of the cross-modal verification task is to verify if, in a given single sample with both a face 
                and voice, both belong to the same identity. In addition, we analyze 
                the impact of multiple of languages on cross-modal verification task.
            </p>
            <div class="figures">
                <img src="./static/images/challenge_task_diag.jpg">
                <p>Figure 2: Diagram explaining cross-modal verification and matching task in face-voice association.</p>
            </div>
        </div>

        <!-- Evaluation Metrics -->
        <div class="descriptions" id="evaluation" style="background-color: var(--tertiary-color); padding-bottom: 2vh;">
            <p class="description-headings">EVALUATION METRICS</p>
            <p>
                We are considering <b>Equal Error Rate (EER)</b> as the metric for 
                evaluating the challenge performance. We expect the challenge participants to submit a output score 
                file for every test pairs to indicate how confident the system believes to have a match between the 
                face and voice or in other words, the face and voice belongs to the same person. The higher the 
                score is, the larger is the confidence of being the face and voice from the same person. In real-world 
                applications, people may set a threshold to determine the if the pair belongs to same or different 
                person as binary output. With the threshold higher, the false acceptance rate (FAR) will become lower, 
                and the false rejection rate (FRR) will become higher. The EER is that optial point when both the errors 
                FAR and FRR are equal. Therefore, EER becomes suitable to evaluate the performance of systems than the 
                conventional accuracy since it independent of the threshold. Finally, the lower the EER it can characterize 
                a better system.
                For more information please see <a href="https://arxiv.org/abs/2404.09342">evaluation plan.</a>
            </p>
        </div>

        <!-- Submission -->
        <div class="descriptions" id="submission">
            <p class="description-headings">SUBMISSION</p>
            <p>
                Within the directory containing the submission files, use
                zip archive.zip *.txt and do not zip the folder. Files should be
                named as:
                <ul>
                <code>
                    <li>sub score English heard.txt</li>
                    <li>sub score English unheard.txt</li>
                    <li>sub score Urdu heard.txt</li>
                    <li>sub score Urdu unheard.txt</li>
                </code>
                </ul>
                Files are submitted through Codalab in the evaluation phase
                3 times per day.
            </p>

            <p>
                We provide both train and test splits for v2 of MAV-Celeb dataset. 
                Participants can use this split for fine-tuning their method. However, for v1 the test files 
                are in format as below:
                <ul>
                <code>
                    <li>ysuvkz41 voices/English/00000.wav faces/English/00000.jpg</li>
                    <li>tog3zj45 voices/English/00001.wav faces/English/00001.jpg</li>
                    <li>ky5xfj1d voices/English/00002.wav faces/English/00002.jpg</li>
                    <li>yx4nfa35 voices/English/01062.wav faces/English/01062.jpg</li>
                    <li>bowsaf5e voices/English/01063.wav faces/English/01063.jpg</li>
                </code>
                </ul>
                We have kept the ground truth for fair evaluation during FAME challenge. Participants are expected 
                to compute and submit a text file including the id and L2 Scores in the following format:
                <ul>
                <code>
                    <li>ysuvkz41 0.9988</li>
                    <li>tog3zj45 0.1146</li>
                    <li>ky5xfj1d 0.6514</li>
                    <li>yx4nfa35 1.5321</li>
                    <li>bowsaf5e 1.6578</li>
                </code>
                </ul>
                The overall score will be computed as:<br>
                <p style="text-align: center;"><code><b>Overall Score = (Sum of all EERs) / 4</b></code></p>
                <br>
                Link to Codalab: <a href="https://codalab.lisn.upsaclay.fr/competitions/18534">Codalab</a>
            </p>
        </div>

        <!-- Registration -->
        <div class="descriptions" id="registration">
            <p class="description-headings">REGISTRATION</p>
            <p>We welcome participants to apply for the “FAME Challenge 2024” by expressing their interest via google forms at 
                <span><a href="https://docs.google.com/forms/d/e/1FAIpQLScBPcRr_ReZHpGE1mxE1Zajjyg5chwOzQtqhV3KCmKBSoOKBA/viewform?gxid=-8203366">this link</a></span>.</p>
            <p>For any queries please contact us at our email <a href="mailto:mavceleb@gmail.com">mavceleb@gmail.com</a>.</p>
        </div>

        <!-- Timeline -->
        <div class="descriptions" id="registration" style="background-color: var(--tertiary-color); padding-bottom: 2vh;">
            <p class="description-headings">TIMELINE</p>
            <ul>
                <li>Registration Period: 15 April-1 June 2024</li>
                <li>Progress Phase: 15 April- 14 June 2024</li>
                <li>Evaluation Phase: 15 June- 21 June 2024</li>
                <li>Challenge Results: 27 June 2024</li>
                <li>Submission of System Descriptions: 30 June 2024</li>
                <li>ACM Grand Challenge Paper Submission: <b>TBA</b></li>
            </ul>
        </div>

        <!-- Organizers -->
        <div class="descriptions" id="organizers">
            <p class="description-headings">ORGANIZERS</p>
            <p>
                <a href="https://scholar.google.com/citations?user=uyhEJ5IAAAAJ&hl=en">Muhammad Saad Saeed</a> - <span  style="font-size: 12px;">Swarm Robotics Lab (SRL)-NCRA, University of Engineering and Technology Taxila</span> <br>
                <a href="https://shahnawazgrewal.github.io/">Shah Nawaz</a> - <span  style="font-size: 12px;">Institute of Computational Perception, Johannes Kepler University Linz, Austria</span> <br>
                <a href="#">Muhammad Salman Tahir</a> - <span  style="font-size: 12px;">Swarm Robotics Lab (SRL)-NCRA, University of Engineering and Technology Taxila</span> <br>
                <a href="https://sites.google.com/view/rohankumardas">Rohan Kumar Das</a> - <span  style="font-size: 12px;">Fortemedia Singapore, Singapore</span> <br>
                <a href="https://scholar.google.com/citations?user=nFxWrXEAAAAJ&hl=en">Muhammad Zaigham Zaheer</a> - <span  style="font-size: 12px;">Mohamed bin Zayed University of Artificial Intelligence</span> <br>
                <a href="https://hcai.at/persons/moscati/">Marta Moscati</a> - <span  style="font-size: 12px;">Institute of Computational Perception, Johannes Kepler University Linz, Austria</span> <br>
                <a href="http://www.mschedl.eu/">Markus Schedl</a> - <span  style="font-size: 12px;">Institute of Computational Perception, Johannes Kepler University Linz, Austria | Human-centered AI Group, AI Lab, Linz Institute of Technology, Austria</span> <br>
                <a href="https://mbzuai.ac.ae/study/faculty/muhammad-haris-khan/">Muhammad Haris Khan</a> - <span  style="font-size: 12px;">Mohamed bin Zayed University of Artificial Intelligence</span> <br>
                <a href="https://mbzuai.ac.ae/study/faculty/karthik-nandakumar/">Karthik Nandakumar</a> - <span  style="font-size: 12px;">Mohamed bin Zayed University of Artificial Intelligence</span> <br>
                <a href="https://fms.uettaxila.edu.pk/Profile/haroon.yousaf">Muhammad Haroon Yousaf</a> - <span  style="font-size: 12px;">Swarm Robotics Lab (SRL)-NCRA, University of Engineering and Technology Taxila</span> <br>
                </p>
        </div>

        <!-- footer -->
        <div class="footer">

        </div>
    </div>
    

</body>
</html>
